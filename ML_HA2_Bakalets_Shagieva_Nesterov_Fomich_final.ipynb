{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f85c21-c336-449f-9222-6e29e4b1c5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette('muted')\n",
    "sns.set_color_codes('muted')\n",
    "sns.set_style('white')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5ed0b3-1d25-4902-8a7f-5b0c8b227879",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adb239",
   "metadata": {},
   "source": [
    "Group: Bakalets D., Nesterov M., Shagieva Z. and Fomich A.\n",
    "\n",
    "Competition and task description are available on [kaggle](https://www.kaggle.com/competitions/gsom-23sm1-ml-hometask-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69ae99-187b-47cd-b729-c3dad2b8e1b7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5efad4-5149-475e-a0dd-be3e4fc01bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('corona_train.csv', encoding='ISO-8859-1', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd58d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= pd.read_csv('corona_test.csv', encoding='ISO-8859-1', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a1c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['type']='train'\n",
    "test['type']='test'\n",
    "test['Sentiment']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a3dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f97a18-e4b9-4e63-8b30-9291f8520061",
   "metadata": {},
   "source": [
    "## Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ec0c8c-8a8b-4af4-af17-e79f7c7d61ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 44955 entries, 0 to 17981\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   UserName       44955 non-null  int64 \n",
      " 1   ScreenName     44955 non-null  int64 \n",
      " 2   Location       35531 non-null  object\n",
      " 3   TweetAt        44955 non-null  object\n",
      " 4   OriginalTweet  44955 non-null  object\n",
      " 5   Sentiment      44955 non-null  object\n",
      " 6   type           44955 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train.drop_duplicates(subset='OriginalTweet').info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70354eb4",
   "metadata": {},
   "source": [
    "### Text correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1bc61cca-89ed-442f-b123-47350ff1c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning \n",
    "import re\n",
    "\n",
    "def tweet_cleaner(tweet):\n",
    "    \n",
    "    # remove urls\n",
    "    tweet = re.sub(r'http\\S+', ' ', tweet)\n",
    "    \n",
    "    # remove html tags\n",
    "    #tweet = re.sub(r'<.*?>', ' ', tweet)\n",
    "    \n",
    "    # remove digits\n",
    "    #tweet = re.sub(r'\\d+', ' ', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#\\w+', ' ', tweet)\n",
    "    \n",
    "    # remove mentions\n",
    "    tweet = re.sub(r'@\\w+', ' ', tweet)\n",
    "    \n",
    "    # remove whitespaces\n",
    "    tweet = ' '.join(tweet.split())\n",
    "\n",
    "    return tweet\n",
    "    \n",
    " \n",
    "train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: tweet_cleaner(x)) \n",
    "train['CleanTweet'] = train['OriginalTweet'].apply(lambda x: x.replace('\\n', ' '))\n",
    "train['CleanTweet'] = train['CleanTweet'].str.lower()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6c377c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dariabakalets/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a9f95f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/dariabakalets/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d466f39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dariabakalets/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7e662fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "997476bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dariabakalets/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bad7666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Split the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Lemmatize each word and join them back into a string\n",
    "    return ' '.join([wnl.lemmatize(word, get_wordnet_pos(word)) for word in words])\n",
    "\n",
    "# Apply the lemmatization function to the text data\n",
    "train['CleanTweet'] = train['CleanTweet'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6fd906e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "train['CleanTweet'] = train['CleanTweet'].apply(lambda x: cleaning_repeating_char(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7303261",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7fdd9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text=train[train['type']=='train']['CleanTweet']\n",
    "X_test_text=train[train['type']=='test']['CleanTweet']\n",
    "y_train_text=train[train['type']=='train']['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b10a00",
   "metadata": {},
   "source": [
    "#### Find the best params for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aaaf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba20f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "params_grid = dict(\n",
    "                   C=[1, 10, 100],\n",
    "                   gamma=[0.1, 0.01, 0.001],\n",
    "                   kernel=['linear', 'rbf'],\n",
    "                   min_df=[.0001, .0005, .0007, .001, .005, .01], \n",
    "                   max_df=[.7, .75, .8, .85, .9])\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in tqdm(ParameterGrid(params_grid)):\n",
    "    pipe = Pipeline(steps=[('tf_idf_vec', TfidfVectorizer(token_pattern=r'[A-Za-z]{2,}',\n",
    "                                                          max_df=params['max_df'],\n",
    "                                                          min_df=params['min_df'],\n",
    "                                                          stop_words='english')\n",
    "                            ),\n",
    "                           ('classifier', SVC(C=params['C'],\n",
    "                                               gamma=params['gamma'],\n",
    "                                               kernel=params['kernel']))\n",
    "                           ])\n",
    "\n",
    "    pipe.fit(X_train_text['CleanTweet'], y_train_text)\n",
    "\n",
    "    pipe_preds_train = pipe.predict(X_train_text.CleanTweet)\n",
    "    pipe_preds_test = pipe.predict(X_test_text.CleanTweet)\n",
    "\n",
    "    results.append(dict(\n",
    "\n",
    "        params=params,\n",
    "\n",
    "        precision_train=precision_score(y_true=y_train_text, y_pred=pipe_preds_train, average='macro'),\n",
    "\n",
    "\n",
    "        recall_train=recall_score(y_true=y_train_text, y_pred=pipe_preds_train, average='macro'),\n",
    "\n",
    "\n",
    "        f1_train=f1_score(y_true=y_train_text, y_pred=pipe_preds_train, average='macro'),\n",
    "\n",
    "\n",
    "        accuracy_train=accuracy_score(y_true=y_train_text, y_pred=pipe_preds_train),\n",
    "\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results.sort_values('accuracy_train', ascending=False).head(10).style.bar(vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a2f53",
   "metadata": {},
   "source": [
    "Use the best params for final calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "584a6c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [02:29<00:00, 149.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "params_grid = dict(min_df=[0.0005],\n",
    "                   max_df=[0.85],\n",
    "                   C=[10],\n",
    "                   gamma=[0.1],\n",
    "                   kernel=['rbf'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in tqdm(ParameterGrid(params_grid)):\n",
    "    pipe = Pipeline(steps=[('tf_idf_vec', TfidfVectorizer(token_pattern=r'[A-Za-z]{2,}',\n",
    "                                                          max_df=params['max_df'],\n",
    "                                                          min_df=params['min_df'],\n",
    "                                                          stop_words='english')\n",
    "                            ),\n",
    "                           ('classifier', SVC(C=params['C'],\n",
    "                                               gamma=params['gamma'],\n",
    "                                               kernel=params['kernel']))\n",
    "                           ])\n",
    "\n",
    "    pipe.fit(X_train_text, y_train_text)\n",
    "\n",
    "    pipe_preds_train = pipe.predict(X_train_text)\n",
    "    pipe_preds_test = pipe.predict(X_test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(dict(\n",
    "params=params,\n",
    "precision_train=precision_score(y_true=y_train_text, y_pred=pipe_preds_train, average='macro'),\n",
    "recall_train=recall_score(y_true=y_train_text, y_pred=pipe_preds_train, average='macro'),\n",
    "f1_train=f1_score(y_true=y_train_text, y_pred=pipe_preds_train, average='macro'),\n",
    "accuracy_train=accuracy_score(y_true=y_train_text, y_pred=pipe_preds_train),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "559857ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pipe_preds_test).to_csv('result_tweet.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f810b4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_29022_row0_col0 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #d65f5f 82.8%, transparent 82.8%);\n",
       "}\n",
       "#T_29022_row0_col1 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #d65f5f 81.2%, transparent 81.2%);\n",
       "}\n",
       "#T_29022_row0_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #d65f5f 81.9%, transparent 81.9%);\n",
       "}\n",
       "#T_29022_row0_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #d65f5f 81.4%, transparent 81.4%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_29022\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_29022_level0_col0\" class=\"col_heading level0 col0\" >precision_train</th>\n",
       "      <th id=\"T_29022_level0_col1\" class=\"col_heading level0 col1\" >recall_train</th>\n",
       "      <th id=\"T_29022_level0_col2\" class=\"col_heading level0 col2\" >f1_train</th>\n",
       "      <th id=\"T_29022_level0_col3\" class=\"col_heading level0 col3\" >accuracy_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_29022_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_29022_row0_col0\" class=\"data row0 col0\" >0.827942</td>\n",
       "      <td id=\"T_29022_row0_col1\" class=\"data row0 col1\" >0.811567</td>\n",
       "      <td id=\"T_29022_row0_col2\" class=\"data row0 col2\" >0.818528</td>\n",
       "      <td id=\"T_29022_row0_col3\" class=\"data row0 col3\" >0.814185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc5a1e629a0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results.sort_values('accuracy_train', ascending=False).head(10).style.bar(vmin=0, vmax=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
